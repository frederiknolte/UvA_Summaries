\section{Combining models}
\begin{itemize}
	\item Improve performance by combining different models
	\item For example, we can train $L$ different models and take their average as prediction (called committee)
	\item Alternatively, we can also make the choice of which model we should use for an input $\bm{x}$ dependent on $\bm{x}$. This example includes Mixtures of experts
	\item \textbf{Bayesian model averaging vs. model combination methods}
	\begin{itemize}
		\item In Bayesian model averaging, the entire dataset is generated by a single model. We are just unsure which one it is. The likelihood of the data is thus:
		$$p(\bm{X}) = \sum_{h=1}^{H} p(\bm{X}|h)p(h)$$
		\item In contrast, model combination methods consider that different data points can be generated by different components. So, every data point has its own latent variable $\bm{z}_n$. The likelihood is here given by:
		$$p(\bm{X}) = \prod_{n=1}^{N}\sum_{\bm{z}_n} p(\bm{x}_n|\bm{z}_n)p(\bm{z}_n)$$
		Example methods include Gaussian mixture models and Mixture of experts.
	\end{itemize}
\end{itemize}
\subsection{Committees}
\begin{itemize}
	\item We can motivate the idea of committees by the bias-variance decomposition: when we average over models that were trained on different datasets, we are able to reduce the variance of the model's predictions. Thus, by using complex models with low bias error, we can improve the performance by reducing the variance through averaging
	\item Averaging is therefore only effective if models are complex enough to overfit
	\item However, in practice, we have only one dataset on which we train $\Rightarrow$ introduce variability between the models within the committee by various methods
\end{itemize}
\subsubsection{Bootstrap aggregation}
\begin{itemize}
	\item Suppose we have a dataset $\bm{X} = \left[\bm{x}_1, ..., \bm{x}_N\right]^T$
	\item \textbf{Bootstrapping dataset}: we create $B$ datasets by sampling $N$ datapoints \textit{with replacement} from the original dataset $\bm{X}$. So, in $\bm{X}_b$, some points will occur more than once and others might be absent
	\item For doing regression with this method, we train $B$ models on their corresponding dataset, and use the average prediction for a new point:
	$$y(\bm{x}) = \frac{1}{B}\sum\limits_{b=1}^{B} y_b(\bm{x})$$ where $$ y_b(\bm{x}) = h(\bm{x}) + \epsilon_b(\bm{x})$$ where $h(\bm{x})$ is ground truth and $\epsilon_b(\bm{x})$ is error of model $b$
	\item This is called bootstrap aggregation or also \textit{bagging}
	\item The average error made by model $b$ is $E_{\bm{x}}(\{ y_b(\bm{x}) - h(\bm{x}) \}^2) = E_{\bm{x}}(\epsilon_b(\bm{x})^2)$ 
	\item Average error made by the $b$ models individually: $E_{\text{AV}} = \frac{1}{B}\sum_{b=1}^{B} \mathbb{E}_{\bm{x}}\left[\epsilon_b(\bm{x})^2\right]$. In contrast, for the committee, we expect an error of:
	$$E_{\text{COM}} = E_{\bm{x}}\left[ \left\{ \dfrac{1}{B} \sum_{b=1}^{B} y_b(\bm{x}) - h(\bm{x}) \right\}^2 \right] = \mathbb{E}_{\bm{x}}\left[\left\{\frac{1}{B}\sum\limits_{b=1}^{B}\epsilon_b(\bm{x})\right\}^2\right]$$
	\item If all models would be independent (which they are not because of using very similar datasets), we would reduce the expected error by factor $B$. In practice, we can at least guarantee that $E_{\text{COM}}\leq E_{\text{AV}}$
	\item Still, bias error cannot be reduced by bagging!
\end{itemize}
\subsubsection{Feature bagging}
\begin{itemize}
	\item Similar to bagging, but based on features: sample a subset of \textit{features} of length $r<D$ for each learner. 
	$$\bm{x} = \left[x_1, x_2, \dots, x_D\right]^T\Rightarrow \bm{\tilde{x}} = \left[x_1, x_3, x_5, x_{D-1}\right]^T$$
	\item Also called \textit{random subspace method}
	\item Works especially well if features are uncorrelated and/or if the number of features is much larger than the number of training points
	\item Causes learners to not over-focus on features that are overly predictive for training set but do not generalize  
	\item Decision trees with bootstrapping and random subspaces lead to random forests
\end{itemize}
\subsubsection{Boosting}
\begin{itemize}
	\item Use a set of simple individual models (also called weak classifiers) which even can be only slightly better than random
	\item In the following description, we concentrate on boosting for classification, but it can also be used for regression
	\item \textbf{AdaBoost}: adaptive boosting
	\item Base classifiers are trained in a sequence where every model uses a weighted form of the dataset
	\item The weight coefficients are associated to the performance of the previous models
	\item In the end, a prediction is based on the (weighted) majority voting scheme:
	$$Y_M(\bm{x}) = \text{sign}\left(\sum\limits_{m=1}^{M}\alpha_m y_m(\bm{x})\right)$$
	\item AdaBoost algorithm:
	\begin{enumerate}
		\item Initialize weights $w_n = 1/N$ for all $n=1, ...,N$
		\item For all models $m=1,...,M$ sequentially:
		\begin{enumerate}
			\item Fit classifier $y_m(\bm{x})$ to minimize $J_m = \sum\limits_{n=1}^{N} w_n^{(m)} \bm{I}[y_m(\bm{x}_n)\neq t_n]$
			\item Compute weighted error rate $\epsilon_m = \frac{\sum_{n=1}^{N}w_n^{(m)}\bm{I}[y_m(\bm{x}_n)\neq t_n]}{\sum_{n=1}^{N}w_n^{(m)}}$ and $\alpha_m = \ln\left(\frac{1-\epsilon_m}{\epsilon_m}\right)$
			\item Update weights $w_n^{(m+1)} = w_n^{(m)}\exp\left\{\alpha_m \bm{I}\left[y_m(\bm{x}_n)\neq t_n\right]\right\}$
		\end{enumerate}
		\item Make predictions $Y_M(\bm{x}) = \text{sign}\left(\sum\limits_{m=1}^{M}\alpha_m y_m(\bm{x})\right)$
	\end{enumerate}
	\item Note that the weight in the prediction ($\alpha_m$) is based on the average error it has on the weighted training dataset (greater weights for more accurate models)
	\item When taking a huge number of basis models (large $M$), we can easily overfit
	\item Interpretation/Derivation of AdaBoost: minimizing exponential error function sequentially ($E_m = \sum_{n=1}^{N}\exp\left(-t_n f_m(\bm{x}_n)\right)$)
	\item \textbf{Advantages}: simple boosting algorithm
	\item \textbf{Disadvantages}: very sensitive to outliers (for which $t_n y_m(\bm{x})$ is large negative), no probabilistic interpretation, not easy to generalize to $K > 2$
\end{itemize}
\subsection{Decision trees}
\begin{itemize}
	\item Split input space into rectangles which are aligned along the axes (parallel to axes)
	\item We use sequential binary decisions which can be summarized in a tree structure
	\item Used for classification and regression
	\item For regression the prediction of a new datapoint is the average of the training values in the bucket
	\item For classification the predicted class is determined by majority vote of training samples in the bucket
	\item \textbf{Advantages}: interpretable, combining with boosting strongly increases performance
	\item \textbf{Disadvantages}: Not state-of-the-art, large trees easily overfit but small trees underfit (can be prevented by training large trees and sequentially removing nodes that reduce the error the least), short-sighted because of greedy
	\item Tree building process is recursively by minimizing the squared error (for regression). At each iteration, we add the feature boundary that reduces the error the most
	\item Stop criteria can be for example min. number of data points in region, depth/height,... or decrease of loss is lower than certain threshold
	\item \textit{Pruning}: give a penalty to trees with large number of leafs to prevent unnecessary overfitting 
	\item \textbf{Random forests}: By combining bootstrapping and feature bagging, we ensure that the models uses different features to build the trees. Thus, the models are less correlated and probably result in better accuracies.
\end{itemize}