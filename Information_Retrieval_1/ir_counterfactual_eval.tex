\section{Counterfactual Evaluation and Learning to Rank}
\begin{itemize}
	\item The term \textit{counterfactual} relates to \textit{off-policy} learning in RL
\end{itemize}
\subsection{Counterfactual Evaluation}
\begin{itemize}
	\item Evaluation is important before deploying a system
	\item \textit{Counterfactual evaluation}: perform offline evaluation of online metrics given online data from another already deployed system
	\item Note that we only have partial information feedback, and no complete supervision. Only for the chosen action, we know the feedback signal/user utility. Thus, the "correct"/optimal action is unknown (also called "bandit feedback" as it was sampled from only one arm)
	\item Counterfactual Evaluation: Full information
	\begin{itemize}
		\item we know true relevance labels $y(d_i)$ for all $i$ and thus can compose additive IR metric as:
		$$ \Delta(f_\theta, D, y) = \sum_{d_i \in D} \lambda (\mathrm{rank}(d_i | f_\theta, D)) \cdot y(d_i) $$
		\item where $\lambda$ is a weighting function, e.g.
		\begin{itemize}
			\item average relevant position ARP: $\lambda(r) = r$
			\item discounted cumulative gain DCG: $\lambda(r) = \dfrac{1}{\log_2 (1 + r)}$
			\item precision at $k$: $\lambda(r) = \dfrac{\mathbf{1} [r \leq k]}{k}$
		\end{itemize}
	\end{itemize}
	\item Counterfactual Evaluation: partial Information
	\begin{itemize}
		\item we do not know the true relevance labels $y(d_i)$ but can observe implicit biased and noisy feedback through clicks
		\item Naive Estimator:
		\begin{itemize}
			\item use click instead of relevance label
			$$ \Delta_{\mathrm{Naive}}(f_\theta, D, y) = \sum_{d_i \in D} \lambda (\mathrm{rank}(d_i | f_\theta, D)) \cdot y(d_i) $$
			\item even if no noise is present, the estimator $P(c_i = 1 | o_i = 1, y(d_i)) = y(d_i)$ is biased by examination probabilities:
			\begin{align*}
				\mathbb{E}_o [\Delta_{\mathrm{Naive}}(f_\theta,D,c)] &= \mathbb{E}_o \Big[ \sum_{d_i \in D} c_i \cdot \lambda (rank(d_i|f_\theta, D)) \Big] \\
				&= \mathbb{E}_o \Big[ \sum_{d_i \in D} o_i \cdot y(d_i) \cdot \lambda (rank(d_i|f_\theta, D)) \Big] \\	
				&= \sum_{d_i \in D} P(o_i = 1 | R, d_i) \cdot \lambda(rank(d_i |f_\theta, D)) \cdot y(d_i)
			\end{align*}
			\item thus estimator weights documents according to examination probabilities during logging $\rightarrow$ position bias as higher rankings are more likely to be observed
			\item This leads to self-confirming behaviour: documents that were ranked high during logging received a lot of clicks. This leads to incorrectly assuming these documents are more relevant as they are weighted more in the evaluation
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Inverse Propensity Scoring}
\begin{itemize}
	\item accounts for bias:
	$$ \Delta_{IPS}(f_\theta, D, c) = \sum_{d_i \in D} \dfrac{\lambda(rank(d_i |f_\theta, D))}{P(o_i = 1 |R,d_i)} \cdot c_i $$
	where $c_i$ is observed click from the log and $P(o_i = 1 |R,d_i)$ is examination probability in $R$ during logging
	\item this is unbiased of any additive linearly decomposable metric
\end{itemize}
\subsection{Counterfactual Learning to Rank}
\begin{itemize}
	\item Rank-SVM optimizes differentiable upper bound on rank:
	\begin{align*}
		rank(d|f_\theta,D) &= \sum_{d' \in R} \mathbf{1} [f_\theta(d) \leq f_\theta(d')] \\
		&\leq \sum_{d' \in R} \max (1 - (f_\theta(d) - f_\theta(d')), 0) = \overline{rank}(d|f_\theta, D)
	\end{align*}
	Then for average relevance position metric:
	$$ \Delta_{ARP}(f_\theta,D,y) = \sum_{d_i \in D} rank(d_i |f_\theta,D) \cdot y(d_i) $$
	we have an unbiased estimator and differentiable upper bound
	\begin{align*}
		\Delta_{ARP-IPS}(f_\theta,D,y) &= \sum_{d_i \in D} \dfrac{rank(d_i |f_\theta,D)}{P(o_i = 1|R,d_i)} \cdot c_i \\
		&\leq \sum_{d_i \in D} \dfrac{\overline{rank}(d_i |f_\theta,D)}{P(o_i = 1|R,d_i)} \cdot c_i
	\end{align*}
	\item Similar approach for additive metrics:
	If $\lambda$ is monotonically decreasing, then 
	$$rank(d|\cdot) \leq \overline{rank}(d|\cdot) \Rightarrow \lambda(rank(d|\cdot)) \geq \lambda(rank(d|\cdot))$$
	which provides lower bound e.g. for DCG we use:
	$$\dfrac{1}{\log_2 (1 + rank(d|\cdot))} \geq \dfrac{1}{\log_2(1+ \overline{rank}(d|\cdot))}$$
	such that for DCG:
	$$\Delta_{DCG}(f_\theta,D,y) = \sum_{d_i \in D}\log_2(1 + rank(d_i|f_\theta,D))^{-1} \cdot y(d_i)$$
	we obtain unbiased etimator and differentiable lower bound
	\begin{align*}
		\Delta_{DCG-IPS}(f_\theta,D,y) &= \sum_{d_i \in D} \dfrac{\log_2(1 + rank(d_i|f_\theta,D))^{-1}}{P(o_i = 1|R,d_i)} \cdot c_i \\
		&\geq \sum_{d_i \in D} \dfrac{\log_2(1 + \overline{rank}(d_i|f_\theta,D))^{-1}}{P(o_i = 1|R,d_i)} \cdot c_i
	\end{align*}
	\item In total, propensity-weighted LTR takes the following steps:
	\begin{enumerate}
		\item obtain model of position bias
		\item acquire large click-log
		\item for each click in log:
		\begin{enumerate}
			\item compute propensity of click $P(o_i = 1|R,d_i)$
			\item calculate gradient of the bound of the unbiased estimator $\nabla_\theta \Big[ \dfrac{\lambda (\overline{rank}(d_i|f_\theta, D))}{P(o_i = 1|R,d_i)} \Big]$
			\item update model using the gradient
		\end{enumerate}
	\end{enumerate}
\end{itemize}
\subsubsection{Estimating the position bias}
\begin{itemize}
	\item Assumption: observation probability (propensity) only depends on rank of document: $P(o_i = 1|i)$
	\item Use the RandTop-$n$ algorithm:
	\begin{enumerate}
		\item repeat:
		\begin{enumerate}
			\item randomly shuffle top $n$ items
			\item record clicks
		\end{enumerate}
		\item aggregate clicks per rank
		\item normalize to obtain propensities $p_i \propto P(o_i|i)$
	\end{enumerate}
	\begin{itemize}
		\item Disadvantage: user experience suffers extremely when shuffling items randomly
	\end{itemize}
	\item Use RandPair: Choose a pivot rank $k$ and only swap a random other document with the document at this pivot rank
	\item Interventional sets/harvesting: exploit inherent "randomness" in data coming from multiple rankers (e.g. A/B tests in production logs):
	\begin{itemize}
		\item Ranker A produces ranking: $d_1,d_2,d_3,d_4$
		\item Ranker B produces ranking: $d_3,d_2,d_1,d_4$
		\item if you have enough of these interactions, you see enough variations to estimate position bias (i.e. instead of swapping documents by force, use swaps that result from different systems in A/B testing)
	\end{itemize}
	\item Jointly learning and estimating:
	\begin{itemize}
		\item recall how probability of click can be decomposed into relevance probability times observation probability
		\item if we know relevance probability, we can estimate observation probability and vice versa (as seen before in IPS)
		\item It is possible to jointly learn both by iterating:
		\begin{enumerate}
			\item learn optimal ranker given correct propensity model:
			$$ P(c_i = 1 |o_i =1, y(d_i)) = \dfrac{P(c_i = 1 \land o_i = 1|y(d_i),R)}{P(o_i|R,d_i)}$$
			\item learn optimal propensity model given correct ranker
			$$P(o_i|R,d_i) = \dfrac{P(c_i = 1 \land o_i = 1|y(d_i),R)}{P(c_i = 1|o_i = 1, y(d_i))}$$
		\end{enumerate}
		\item can also use EM-algorithm instead (this is what click models do)
	\end{itemize}
\end{itemize}
