\section{Offline evaluation}
\begin{itemize}
	\item Evaluating an IR system without any interaction with user 
	\item Assumption: assessors can tell what is relevant
\end{itemize}
\subsection{Collection-based evaluation}
\begin{itemize}
	\item Approximating user happiness by relevance of the found documents
	\item There are different measures to do that
\end{itemize}
\subsubsection{Traditional Evaluation measures}
\begin{itemize}
	\item Relevance is assumed to be:
	\begin{itemize}
		\item topical (document and query on the same topic)
		\item binary (relevant / not relevant)
		\item independent (relevance of document A does not depend on relevance of document B)
		\item stable (relevance judgements do not change over time)
		\item consistent (judgements are consistent across editors)
		\item complete (no missing judgements)
	\end{itemize}
	\item We can view IR as a (binary) classification problem where every document is either relevant or not with respect to a query
	\item The evaluation is performed by calculating precision ($\frac{TP}{TP+FP}$) and recall ($\frac{TP}{TP+FN}$)
	\item However, the output of an IR system is a ranking and not a binary classification. Thus, we label the first $k$ documents the system proposes as relevant, and other as non-relevant $\implies$ precision/recall $@$ cut-off ($P@k$/$R@k$)
	\item Trade-off between precision and recall: returning more documents increases recall; returning fewer documents often increases precision
	\item The trade-off between precision and recall is task specific. For web searches, we want a high precision on the first few documents, but allow a worse recall (as a user doesn't want to find \textit{all} relevant documents). In contrast, in medicine, we want to have a high recall to not miss an important document
	\item Another way to incorporate precision and recall for ranking is using R/P curves. We start with a cut-off point of 0 where precision is 1 and recall 0. Then we increase the cut-off point one by one and record the new values in the curve (left, Figure~\ref{img:offline_eval_RP_curves}). While the cut-off rank heads to infinity, recall goes to 1 and precision to 0. We interpolate the curves by propagating maximum to the left.
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.4\textwidth]{figures/offline_eval_RP_curves.png}
		\caption{R/P curves for ranking}
		\label{img:offline_eval_RP_curves}
	\end{figure}
	\item When having multiple queries, we would average the RP curves. 
	\item The area under the curve is the average precision which can also be calculated by taking the average of all precision values at ranks with relevant documents.
	\item Usually, a binary scale of whether a document is relevant is not sufficient. For a graded relevance scale, we can use different evaluation measures
	\begin{itemize}
		\item \textbf{Discounted Cumulative Gain (DCG)} - considers the relevance grade and position of every document. The total gain is accumulated at a certain rank $k$:
		$$DCG@k = \sum\limits_{\text{rank} r=1}^{k} \frac{2^{\text{rel}_r} - 1}{\log_2\left(1 + r\right)}$$
		\item The numerator is the non-linear relevance score of the document at rank $r$, and the denominator the discount over ranking position (assumes less utility for the user when document is ranked lower as it is less likely to be examined)
		\item The score highly depends on the best possible ranking for a query. Thus, the DCG can be normalized by the value of the best ranking $\implies$ $0\leq nDCG \leq 1$. This makes it easier to compare scores over different queries
	\end{itemize}
	\item When having only a single relevant document (e.g. navigation), then recall-based measures not appropriate. Instead, the search time for the user is determined by the ranking of the matching result. Therefore, optimize for high ranking of that result e.g. by using reciprocal rank (1/rank of (first) relevant document)
\end{itemize}
\subsubsection{Model-based Evaluation measures}
\begin{itemize}
	\item Another perspective of evaluation is looking at different aspects of possibles metrics. A model-based approach considers the following three components:
	\begin{enumerate}
		\item \textbf{Browsing model} - describes how the user interacts with results, like the probability of a document being clicked/viewed $\Rightarrow p(d)$
		\item \textbf{Model of document utility} - describes how a user derives utility from individual relevant documents. Similar to how to determine the graded relevance scale $\Rightarrow g(d)$.
		\item \textbf{Utility accumulation model} - describes how a user accumulates utility in the course of browsing $\Rightarrow E\left[g(D)\right] = \sum_{r=1}^{\infty}g(d) \cdot p(d)$
	\end{enumerate}
	\item Examples for the browsing models
	\begin{itemize}
		\item \textit{Position-based models} - the chance of observing a document depends on the position in the ranking. We can for example model it by $\Rightarrow p(d_r)=(1-\theta)^{r-1} \theta$. The corresponding utility accumulation is described by Rank-biased Precision (RBP): $RBP = \sum_{r=1}^{\infty}\text{rel}_r (1-\theta)^{r-1} \theta$
		\item \textit{Cascade-based models} - considers $\theta$ as a function of the document at rank $r$. Mostly, the following function is used: $\theta_r = \mathcal{R}(\text{rel}_r) = \frac{2^{\text{rel}_r}-1}{2^{\text{max rel}}}$. The corresponding utility accumulation is the \textit{Expected Reciprocal Relevance}: $ERR@k = \sum\limits_{r=1}^{k} \frac{1}{r} \cdot \theta_r \cdot \prod\limits_{i=1}^{r-1}\left(1 - \theta_i\right)$
	\end{itemize}
\end{itemize}
TODO: add slides on choosing parameter values?
\subsubsection{Novelty and diversity-based measures}
\begin{itemize}
	\item Assume that there is a distribution $P(i|Q)$ over intents for query $Q$ $\rightarrow$ an intent-aware version of a measure is its weighted average over this distribution 
	\item $\alpha$-nDCG is generalization of nDCG that accounts for novelty and diversity
	\begin{itemize}
		\item $\alpha$ is geometric penalty for redundancy
		\item the gain of the document becomes:
		\begin{itemize}
			\item +1 for each intent it is relevant To
			\item $\times(1-\alpha)$ for each document higher in the ranking that intent already appeared in
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Sessions and conversations}
\begin{itemize}
	\item So far focus on one-shot queries
	\item users frequently reformulate query $\rightarrow$ can we measure effectiveness of system over sequence of reformulations (session)? Can we optimize systems to provide better results over a session?
	\item User steps down ranked list of results until decision point and either abandons the search or reformulates
	\item while stepping down or sideways, the user accumulates utility
	\item \textbf{Session DCG:} From two queries with DCGs $\mathrm{DCG}(RL1)$ and $\mathrm{DCG}(RL2)$, we can compute the session DCG (does not consider early abandonment):
	$$
	\mathrm{SessionDCG} = \dfrac{1}{\log_c (1 + c -1)}\mathrm{DCG}(RL1) + \dfrac{1}{\log_c (2 + c -1)}\mathrm{DCG}(RL2)
	$$
	\item \textbf{Expected Global Utility:} $\Omega$ is space of all paths, $P(\omega)$ is prob of user following path $\omega \in \Omega$, $U(\omega)$ is utility of $\omega$ $\rightarrow$ esM $= \sum_{\omega \in \Omega} P(\omega) U(\omega)$
	\begin{itemize}
		\item User steps down ranking and stops based on stochastic process $\rightarrow$ reformulates query
		\item $P(\omega) = P(r_1, r_2, \dots, r_K)$ where $r_i$ is stopping and reformulation point in ranking $i$
		\item use geometric distribution to model stopping/reformulation behavior
	\end{itemize}
\end{itemize}
\subsubsection{Collection construction}
\begin{itemize}
	\item To evaluate a system offline, we need labels of whether a document is relevant with respect to a query (or graded scale) $\Rightarrow$ labels are created by humans
	\item First step is to generate a huge document collection, and generate a set of topics/queries that should be evaluated. Mostly, queries are selected from very frequent, common and rare query bin sets of highly-used search engines 
	\item To be able to calculate measures like recall, we need to find all relevant documents in the collection. Can be done either deterministically or stochastically
	\begin{itemize}
		\item \textbf{Depth-k pooling} - deterministic, standard method. Apply $M$ IR systems and take the union of the $k$ top results of all $M$ systems. This set of documents is labeled by humans, and all others are considered as not relevant. Note that we need the $M$ systems to be different/take another perspective on the data so that they don't find all the same documents. Otherwise, future IR algorithms can find relevant documents that the others haven't found yet and will be punished for that. $k$ is task specific, but a value of $100$ has shown to be sufficient
		\item \textbf{Random Sampling} - stochastic method. Simplest approach is for a query $q$, just sample a small set of documents out of the whole corpus and label those. Otherwise are considered as unlabeled, thus neglected in evaluation. Problem: significant sparsity of relevant documents in the corpus. 
		\item \textbf{Stratified Random Sampling} Search engines bring relevant to the top -> oversample from top, undersample from bottom -> cannot take simple average but need to take weighted average
	\end{itemize}
\end{itemize}
\subsection{Challenges of offline evaluation}
\label{sec:offline_eval_problems}
\begin{itemize} 
	\item Expensive and slow to collect new data
	\item Ambiguous queries are particularly hard to judge realistically (what intent is most popular?). Particularly hard for personalized searches
	\item Judges need to correctly appreciate uncertainty/allow different intents
	\item How to identify when relevance changes (temporal, query intent changes, ...)?
\end{itemize}
\subsection{Comparative evaluation}
\begin{itemize}
	\item How do we compare different retrieval systems? Is the difference only due to random noise? $\implies$ Statistical significance test
	\item Two hypotheses where we want to prove that $H_0$ is wrong: $$H_0: \text{MAP}_E - \text{MAP}_P = 0, \hspace{4mm} H_1: \underbrace{\text{MAP}_E - \text{MAP}_P \neq 0}_{\text{two-sided}} \text{\hspace{2mm}or\hspace{2mm}}\underbrace{\text{MAP}_E - \text{MAP}_P > 0}_{\text{one-sided}}$$
	\item Compute the $p$-value that describes the probability of observing the test data given that $H_0$ is valid (low $p$-value disprove null hypothesis)
\end{itemize}
\subsection{Informativeness}
\begin{itemize}
	\item if you care about something, you should optimize for that
	\item this is not necessarily true if you don't have enough training data
	\item then, optimize for different metric that is however more informative for the algorithm
	\item an informative metric is a metric such that if I tell you the value, you can tell me the ranking for it
	\item e.g. given average precision value, infer probability of relevance of document at rank $i$: maximize entropy under the constraint that the average precision is at is
\end{itemize}
