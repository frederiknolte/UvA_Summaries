\section{Neural Retrieval Models}
\subsection{Distributed Word Representations}
\begin{itemize}
	\item Latent, dense vector representation to model semantic similarity/relations
	
\end{itemize}
\subsubsection{Skip-gram}
\begin{itemize}
	\item \textbf{Skip gram}: learn to predict neighboring words in a small context window
	\item Model probability by similarity between word and context vectors (two matrices):
	$$p(w_k|w_j) = \frac{\exp\left(c_k \cdot v_j\right)}{\sum_{i\in|V|} \exp\left(c_i \cdot v_j\right)}$$
	\item Denominator can be computationally expensive if vocabulary is quite large. Thus, we can approximate it by taking just a few negative examples $\implies$ negative sampling
	\item Overall, skip gram will learn two representations for each word (context $C$ and target words $V$) from which me most likely only use $V$ (visualized in Figure~\ref{img:neural_ir_skip_gram})
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.4\textwidth]{figures/neural_ir_skip_gram.png}
		\caption{Visualization of skip gram method for learning word representations}
		\label{img:neural_ir_skip_gram}
	\end{figure}
	\item Skip gram shows to capture relational meaning (\texttt{KING - MAN + WOMAN = QUEEN})
\end{itemize}
\subsection{Compositionality}
\begin{itemize}
	\item To match queries and documents in the embedding space, we need to combine the words in each $\implies$ compositionality
\end{itemize}
\subsubsection{Aggregate word vectors}
\begin{itemize}
	\item Apply simple rules/arithmetic to combine word vectors
	\item Example: \textbf{Dual Embedding Space Model} 
	\begin{itemize}
		\item represent a document by the centroid of its word vectors $\bm{\overline{D}} = \frac{1}{|D|}\sum_{\bm{d}_j \in D} \frac{\bm{d}_j}{||\bm{d}_j||}$
		\item The query-document similarity is the average over query words of cosine similarity:\\ $\text{DESM}(Q,D) = \frac{1}{|Q|}\sum_{q_i \in Q} \frac{\bm{q}_i^T \bm{\overline{D}}}{||\bm{q}_i|| \cdot ||\bm{\overline{D}}||}$
		\item We can also use both the IN (word) and OUT (context) embeddings from skip-gram to optimize matching. What worked best was using IN representations for the query and OUT for document
		\item In the ranking system, we either first rank documents by BM25 and rerank top $N$ with DESM, or use a linear combination of both scores
	\end{itemize}
\end{itemize}
\subsubsection{Tune and Aggregate word vectors}
\begin{itemize}
	\item Learn task-specific representations and not rely on pure skip-gram
	\item \textbf{Paragraph2vec}
	\begin{itemize}
		\item Generalizes word2vec to whole documents by embedding them in a fixed-size vector
		\item Two different approaches. First is \textit{distributed memory}:
		\begin{itemize}
			\item We are trying to predict the next word based on a few previous context word \textit{and} a paragraph embedding. 
			\item Both the word and paragraph embeddings are learned during this process
			\item Input embeddings can either be concatenated or averaged (commonly first one is applied)
			\item Visualization in Figure~\ref{img:neural_models_distributed_memory}
			\begin{figure}[ht]
				\centering
				\includegraphics[width=0.3\textwidth]{figures/neural_models_distributed_memory.png}
				\caption{Distributed memory model. }
				\label{img:neural_models_distributed_memory}
			\end{figure}
		\end{itemize}
		\item Second method: \textit{Distributed bag of words}
		\begin{itemize}
			\item In this approach, we don't consider the context words but try to predict all possible words in the paragraph given the embedding vector
			\item This is done by sampling a random word at every SGD iteration from the small text windows, and train the classifier on predicting this word
			\item Thus, we optimize the embedding regarding representing the word distribution in the paragraph
			\item The distributed BOW is visualized in Figure~\ref{img:neural_model_distributed_BOW}
			\begin{figure}[ht]
				\centering
				\includegraphics[width=0.3\textwidth]{figures/neural_model_distributed_BOW.png}
				\caption{Distributed BOW model. }
				\label{img:neural_model_distributed_BOW}
			\end{figure}
		\end{itemize}
		\item When having new query, no embedding vector exists yet
		\item Given a query
		\begin{itemize}
			\item Fix the word matrix $W$
			\item Add a (randomly initialized) column to the document matrix $D$ corresponding to the query representation
			\item Update $D$ using gradient descent
			\item Get the vector representation of the query from the updated matrix $D$
		\end{itemize}
	\end{itemize}
\end{itemize}